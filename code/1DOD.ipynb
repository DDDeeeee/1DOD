{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3ec2c1",
   "metadata": {},
   "source": [
    "The final version of the code was lost, so an early version is provided here.   \n",
    "There may be some differences and bugs, so please modify it as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffdc20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import jieba\n",
    "import shutil\n",
    "import segeval\n",
    "import datasets\n",
    "import numpy as np\n",
    "import jieba.posseg\n",
    "import pandas as pd\n",
    "import jieba.analyse\n",
    "from torch import nn\n",
    "from datasets import Dataset\n",
    "from pandas import DataFrame\n",
    "from typing import List, Dict\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import DBSCAN\n",
    "from torch.nn.functional import pad\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from modelscope.metainfo import Models\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from modelscope.models.builder import MODELS\n",
    "from similarityMetrics import alignmentIndex  # https://github.com/sierra98x/resources\n",
    "from datasets import load_dataset, load_metric\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers.utils import CONFIG_NAME, WEIGHTS_NAME\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from modelscope.utils.constant import ConfigFields, ModelFile\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from datasets import load_dataset, load_metric, ClassLabel, DatasetDict\n",
    "from modelscope.models.nlp.ponet import PoNetPreTrainedModel, PoNetModel\n",
    "from sklearn.metrics import f1_score, classification_report, recall_score\n",
    "from transformers import (BertModel, BertForTokenClassification, BertForMaskedLM, \n",
    "                          VisionEncoderDecoderModel, BertForSequenceClassification, \n",
    "                          BertTokenizerFast, BertConfig, PretrainedConfig)\n",
    "\n",
    "# Replace here\n",
    "%run './1d_iou.ipynb'\n",
    "%run './models.ipynb'\n",
    "\n",
    "device = 'cuda:0'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba2a2db",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d5c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TS_iou_dice_fct = IOU_1D()\n",
    "\n",
    "def trans_data(t):\n",
    "    dev_data_csv = pd.read_csv('./Alimeeting4MUG/'+t+'.csv', sep='\\t')\n",
    "    trans_data = []\n",
    "    for n in range(len(dev_data_csv)):\n",
    "        d1 = dict()\n",
    "        data1 = eval(dev_data_csv['content'][n])\n",
    "        d1['dialog_id'] = str(n)\n",
    "        d1['utterance'] = [i['s'] for i in data1['sentences']]\n",
    "        d1_label = [''] * len(d1['utterance'])\n",
    "        for x in [i['id']-1 for i in data1['topic_segment_ids']]:\n",
    "            d1_label[x] = 1.0\n",
    "        d1['label'] = d1_label\n",
    "        trans_data.append(d1)\n",
    "    return trans_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1830878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    1.0: \"B-EOP\",\n",
    "    '': \"O\",\n",
    "    0.0: \"B-EOP-Invalid\"\n",
    "  }\n",
    "\n",
    "\n",
    "def read_data_streaming(data_, t='train', window_size=100, batch_size=1, epoch=1):\n",
    "    data_piece = []\n",
    "    meeting_length = []\n",
    "    len_200_sum = 0\n",
    "    len_200_num = 0\n",
    "    over_4k_num = 0\n",
    "    for n in tqdm(range(len(data_))):\n",
    "        utterance = data_[n]['utterance']\n",
    "        dialog_id = data_[n]['dialog_id']\n",
    "        label_eop = [id2label[i] for i in data_[n]['label']]\n",
    "        assert len(utterance)==len(label_eop)\n",
    "        label_seg_ids = [-1]+[i for i in range(len(label_eop)) if label_eop[i] != 'O']\n",
    "        for m in range(1, len(label_seg_ids)-1):\n",
    "            #block_sents = utterance[label_seg_ids[m-1]+1:label_seg_ids[m]+1]\n",
    "            #block_label = label_eop[label_seg_ids[m-1]+1:label_seg_ids[m]+1]\n",
    "            block_sents = utterance[label_seg_ids[m-1]+1:label_seg_ids[m-1]+window_size]\n",
    "            len_200_sum += len(''.join(block_sents))\n",
    "            len_200_num += 1\n",
    "            block_label = label_eop[label_seg_ids[m-1]+1:label_seg_ids[m]+window_size]\n",
    "            paragraph = [{'timestamp': str(i+1),\n",
    "                          'speaker': '说话人1',\n",
    "                          'content': block_sents[i]} for i in range(len(block_sents))]\n",
    "            inputs = {'dialog_id': dialog_id,\n",
    "                      'paragraph': paragraph,\n",
    "                      'label':block_label\n",
    "                     }\n",
    "\n",
    "            # 分句（aliMUG不分）\n",
    "            #paragraph_new, newidx2old = dialog_sentence_split(inputs)\n",
    "            paragraph_new = {'dialog_id':inputs['dialog_id'], \n",
    "                             'paragraph_segment_ids':[{'id': i+1} for i in range(len(inputs['paragraph']))], \n",
    "                             'sentences':[{'id': i+1, 'speaker': inputs['paragraph'][i]['speaker'], \n",
    "                                           's': inputs['paragraph'][i]['content']} for i in range(len(inputs['paragraph']))], \n",
    "                             'topic_segment_ids':[]}\n",
    "            pred_raw_dataset = Dataset.from_list([{\"idx\": \"0\", \"content\": paragraph_new}])\n",
    "            pred_eda_one = data_parse_fn(pred_raw_dataset)\n",
    "            predict_dataset = prepare_input_features(pred_eda_one)\n",
    "            # 超过4096\n",
    "            if len(predict_dataset['input_ids'])>1:\n",
    "            #    print(n,m,len(predict_dataset['input_ids']),predict_dataset['input_ids'][-1].index(0),'Over 4096')\n",
    "                over_4k_num += 1\n",
    "            if 0 in predict_dataset['input_ids'][0]:\n",
    "                no_pad_l = predict_dataset['input_ids'][0].index(0)\n",
    "                streaming_input_ids = predict_dataset['input_ids'][0][:no_pad_l]\n",
    "                streaming_attention_mask = predict_dataset['attention_mask'][0][:no_pad_l]\n",
    "                streaming_segment_ids = predict_dataset['segment_ids'][0][:no_pad_l]\n",
    "            else:\n",
    "                streaming_input_ids = predict_dataset['input_ids'][0]\n",
    "                streaming_attention_mask = predict_dataset['attention_mask'][0]\n",
    "                streaming_segment_ids = predict_dataset['segment_ids'][0]\n",
    "            type_id = 0\n",
    "            token_type_ids_seg_sent = []\n",
    "            for i in streaming_input_ids:\n",
    "                token_type_ids_seg_sent.append(type_id)\n",
    "                if i == 21128:\n",
    "                    type_id = 1 - type_id\n",
    "            # 2cls 000...1111...\n",
    "            new_label = [1 if i!=21128 else 0 for i in streaming_input_ids]\n",
    "            seg_1_sent_index = inputs['label'].index(\"B-EOP\") # 这个也算\n",
    "            num_eop = 0\n",
    "            for i in range(len(new_label)):\n",
    "                if new_label[i] == 0:\n",
    "                    num_eop += 1\n",
    "                if num_eop == seg_1_sent_index+1:\n",
    "                    seg_1_token_index = i\n",
    "                    break\n",
    "            new_label = [0]*(seg_1_token_index+1) + [1]*(len(new_label)-seg_1_token_index-1)\n",
    "            # 4096内无分割\n",
    "            if 1 not in new_label:\n",
    "                # print(n,m,'no segment')\n",
    "                # 没有分割点的话seg_1_token_index未定义\n",
    "                new_label = [0]*len(streaming_input_ids)\n",
    "            assert len(streaming_input_ids)==len(token_type_ids_seg_sent)\n",
    "            assert len(streaming_input_ids)==len(new_label)\n",
    "            itasl = [streaming_input_ids,\n",
    "                     token_type_ids_seg_sent,\n",
    "                     streaming_attention_mask,\n",
    "                     streaming_segment_ids,\n",
    "                     new_label]\n",
    "            data_piece.append(itasl)\n",
    "    print(len_200_sum/len_200_num)\n",
    "    while len(data_piece) % batch_size != 0:\n",
    "        data_piece.append(data_piece[random.randint(0,len(data_piece)-1)])\n",
    "    print(t+' length: ', len(data_piece))\n",
    "    print('Over 4096:', over_4k_num)\n",
    "    if t == 'train':\n",
    "        data_piece_epoch = []\n",
    "        for i in range(epoch):\n",
    "            random.shuffle(data_piece)\n",
    "            data_piece_epoch.extend(data_piece)\n",
    "        data_piece = data_piece_epoch\n",
    "    data_list = [\n",
    "        {'input_ids':pad_sequence([torch.tensor(data_piece[j][0]) for j in range(i, i+batch_size)], batch_first=True),\n",
    "         'token_type_ids':pad_sequence([torch.tensor(data_piece[j][1]) for j in range(i, i+batch_size)], batch_first=True),\n",
    "         'attention_mask':pad_sequence([torch.tensor(data_piece[j][2]) for j in range(i, i+batch_size)], batch_first=True),\n",
    "         'segment_ids':pad_sequence([torch.tensor(data_piece[j][3]) for j in range(i, i+batch_size)], batch_first=True),\n",
    "         'labels':pad_sequence([torch.tensor(data_piece[j][4]) for j in range(i, i+batch_size)], batch_first=True)\n",
    "        }\n",
    "        for i in range(0,len(data_piece),batch_size)\n",
    "    ]\n",
    "    return data_list\n",
    "\n",
    "\n",
    "dev_data = read_data_streaming(trans_data(d_t:='dev'), t=d_t, batch_size=1)\n",
    "#train_data = read_data_streaming(trans_data(d_t:='train'), t=d_t, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb4b3c",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c765bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pretrained_model(model, train_loss_avg, eval_loss_avg, num):\n",
    "    model_path = './model_save/'\n",
    "    model_name = model_path+'PoNet_'+str(round(train_loss_avg,4))+'_'+str(round(eval_loss_avg,4))+'_'+str(num)\n",
    "    if not os.path.exists(model_name):\n",
    "        os.makedirs(model_name)\n",
    "    model.save_pretrained(model_name)\n",
    "    torch.save(model.state_dict(), model_name+'/pytorch_model.bin')\n",
    "    return model_name\n",
    "    \n",
    "    \n",
    "def eval_loss():\n",
    "    loss_sum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for n in tqdm(range(len(dev_data))):\n",
    "            outputs = model(**{k:v.to(device) for k,v in dev_data[n].items()})\n",
    "            loss = outputs.loss\n",
    "            loss_sum += float(loss)\n",
    "    loss_avg = loss_sum/len(dev_data)\n",
    "    print('Eval loss: {:.8f}'.format(loss_avg))\n",
    "    return loss_avg\n",
    "    \n",
    "    \n",
    "def train(train_data, lr, eval_steps, do_eval=True, do_test=False):\n",
    "    print('Training steps: {}'.format(len(train_data)),'-'*20,time.strftime('%Y-%m-%d - %H:%M:%S', time.localtime()))\n",
    "    train_loss_sum = 0\n",
    "    save_flag = 0\n",
    "    trian_loss_list = []\n",
    "    eval_loss_list = []\n",
    "    eval_score_list = []\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for n in tqdm(range(len(train_data))):\n",
    "        # eval & test\n",
    "        if n % eval_steps ==0 and n!=0:\n",
    "            trian_loss_list.append(tls:=(train_loss_sum/(n+1)))\n",
    "            print('Train loss: {:.8f}'.format(tls))\n",
    "            if do_eval:\n",
    "                #dev_loss, dev_seg_invalid_num, dev_iou, dev_f1, dev_pred, dev_label = test(model, dev_data, dev_length_label)\n",
    "                dev_loss = eval_loss()\n",
    "                eval_loss_list.append(dev_loss)\n",
    "                model_save_name = save_pretrained_model(model, trian_loss_list[-1], dev_loss, 1)\n",
    "            if do_test: pass\n",
    "            \n",
    "            model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**{k:v.to(device) for k,v in train_data[n].items()})\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_sum += float(loss)\n",
    "    loss_avg = train_loss_sum / len(train_data)\n",
    "    trian_loss_list.append(loss_avg)\n",
    "    print('Train loss: {:.8f}'.format(loss_avg))\n",
    "    dev_loss = eval_loss()\n",
    "    eval_loss_list.append(dev_loss)\n",
    "    model_save_name = save_pretrained_model(model, trian_loss_list[-1], dev_loss, 1)\n",
    "    if do_test: pass\n",
    "    return trian_loss_list, eval_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ccc96",
   "metadata": {},
   "source": [
    "#### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763736c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './nlp_ponet_fill-mask_chinese-base' # https://modelscope.cn/models/iic/nlp_ponet_document-segmentation_topic-level_chinese-base/files\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    use_fast=True,\n",
    "    use_auth_token=False,\n",
    ")\n",
    "tokenizer.padding_side == \"right\"\n",
    "tokenizer.add_special_tokens({'eos_token': '[EOS]'})\n",
    "\n",
    "model = PoNetForTokenClassificationWithIOUloss.from_pretrained(\n",
    "    model_name_or_path=MODEL_DIR,\n",
    "    task=\"token-classification-task\", \n",
    "    #ignore_mismatched_sizes=True,\n",
    "    revision=\"v1.1.0\"\n",
    "    ).to(device)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a2741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_l = [1e-4, 5e-5]\n",
    "batch_size = 2\n",
    "\n",
    "for lr in lr_l:\n",
    "    epochs = 20\n",
    "    print('\\nLearning rate: {}'.format(lr))\n",
    "    train_data = read_data_streaming(trans_data(d_t:='train'), t=d_t, batch_size=batch_size, epoch=epochs)\n",
    "\n",
    "    e_steps = 1484\n",
    "    print('Eval steps: ', e_steps)\n",
    "    \n",
    "    train_loss_l, dev_loss_l = train(train_data, lr, e_steps, do_eval=True)\n",
    "    print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6482ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6cb438",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d64c972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_seg_eval(pred, label):\n",
    "    avg_block_size = round(sum(label)/len(label)/2)\n",
    "    bs = float(segeval.boundary_similarity(pred, label,n_t=avg_block_size))\n",
    "    ss = float(segeval.segmentation_similarity(pred, label,n_t=avg_block_size))\n",
    "    wd = float(segeval.window_diff(pred, label))\n",
    "    pk = float(segeval.pk(pred, label))\n",
    "    As = alignmentIndex(pred, label)\n",
    "    iou = TS_iou_dice_fct.cal_label_pred_iou([sum(label[:x+1])-1 for x in range(len(label))], \n",
    "                                             [sum(pred[:x+1])-1 for x in range(len(pred))])\n",
    "    return {'S':ss, 'B':bs, 'A':As, '1-wd':1-wd, '1-pk':1-pk, 'miou':iou['mean_iou'], 'wiou':iou['weighted_iou']}\n",
    "\n",
    "\n",
    "def test(window_size=100, stop_block_size=23):\n",
    "    data_ = trans_data(d_t:='test')\n",
    "    t=d_t\n",
    "    batch_size=1\n",
    "    window_size = window_size\n",
    "    stop_block_size = stop_block_size\n",
    "\n",
    "    data_piece = []\n",
    "    meeting_length = []\n",
    "    over_4k_num = 0\n",
    "    test_pred_block_length = []\n",
    "    test_label_block_length = []\n",
    "    for n in tqdm(range(len(data_))):\n",
    "        utterance = data_[n]['utterance']\n",
    "        dialog_id = data_[n]['dialog_id']\n",
    "        label_eop = [id2label[i] for i in data_[n]['label']]\n",
    "        assert len(utterance)==len(label_eop)\n",
    "        test_label_seg_ids = [-1]+[i for i in range(len(label_eop)) if label_eop[i] != 'O']\n",
    "        # label block形式\n",
    "        test_label_block = [test_label_seg_ids[i]-test_label_seg_ids[i-1] for i in range(1,len(test_label_seg_ids))]\n",
    "        test_label_block_length.append(test_label_block)\n",
    "\n",
    "        #len(utterance), sum(test_label_block)\n",
    "\n",
    "        block_length = []\n",
    "        block_length_merge = []\n",
    "        while sum(block_length) < len(utterance):\n",
    "            next_seg_start_id = sum(block_length)\n",
    "            #print('start:',next_seg_start_id)\n",
    "            if (len(utterance)-next_seg_start_id)<stop_block_size:\n",
    "                block_length.append(len(utterance)-next_seg_start_id)\n",
    "                block_length_merge.append(len(utterance)-next_seg_start_id)\n",
    "                #print(block_length, sum(block_length),len(utterance))\n",
    "                break\n",
    "            #block_sents = utterance[label_seg_ids[m-1]+1:label_seg_ids[m]+1]\n",
    "            block_sents = utterance[next_seg_start_id: next_seg_start_id+window_size]\n",
    "            paragraph = [{'timestamp': str(i+1),\n",
    "                          'speaker': '说话人1',\n",
    "                          'content': block_sents[i]} for i in range(len(block_sents))]\n",
    "            inputs = {'dialog_id': dialog_id,\n",
    "                      'paragraph': paragraph,\n",
    "                      'label':[]\n",
    "                     }\n",
    "\n",
    "            # 分句（ali不分）\n",
    "            #paragraph_new, newidx2old = dialog_sentence_split(inputs)\n",
    "            paragraph_new = {'dialog_id':inputs['dialog_id'], \n",
    "                             'paragraph_segment_ids':[{'id': i+1} for i in range(len(inputs['paragraph']))], \n",
    "                             'sentences':[{'id': i+1, 'speaker': inputs['paragraph'][i]['speaker'], \n",
    "                                           's': inputs['paragraph'][i]['content']} for i in range(len(inputs['paragraph']))], \n",
    "                             'topic_segment_ids':[]}\n",
    "            pred_raw_dataset = Dataset.from_list([{\"idx\": \"0\", \"content\": paragraph_new}])\n",
    "            pred_eda_one = data_parse_fn(pred_raw_dataset)\n",
    "            predict_dataset = prepare_input_features(pred_eda_one)\n",
    "            # 超过4096\n",
    "            if len(predict_dataset['input_ids'])>1:\n",
    "                #print(n,len(predict_dataset['input_ids']),predict_dataset['input_ids'][-1].index(0),'Over 4096')\n",
    "                over_4k_num += 1\n",
    "            if 0 in predict_dataset['input_ids'][0]:\n",
    "                no_pad_l = predict_dataset['input_ids'][0].index(0)\n",
    "                streaming_input_ids = predict_dataset['input_ids'][0][:no_pad_l]\n",
    "                streaming_attention_mask = predict_dataset['attention_mask'][0][:no_pad_l]\n",
    "                streaming_segment_ids = predict_dataset['segment_ids'][0][:no_pad_l]\n",
    "            else:\n",
    "                streaming_input_ids = predict_dataset['input_ids'][0]\n",
    "                streaming_attention_mask = predict_dataset['attention_mask'][0]\n",
    "                streaming_segment_ids = predict_dataset['segment_ids'][0]\n",
    "            type_id = 0\n",
    "            token_type_ids_seg_sent = []\n",
    "            for i in streaming_input_ids:\n",
    "                token_type_ids_seg_sent.append(type_id)\n",
    "                if i == 21128:\n",
    "                    type_id = 1 - type_id\n",
    "            assert len(streaming_input_ids)==len(token_type_ids_seg_sent)\n",
    "            ## predict\n",
    "            inputs = {'input_ids':torch.tensor([streaming_input_ids]),\n",
    "                      'token_type_ids':torch.tensor([token_type_ids_seg_sent]),\n",
    "                      'attention_mask':torch.tensor([streaming_attention_mask]),\n",
    "                      'segment_ids':torch.tensor([streaming_segment_ids])}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**{k:v.to(device) for k,v in inputs.items()})\n",
    "            # 计算分割位置\n",
    "            pred = torch.argmax(outputs.logits,-1)[0].tolist()\n",
    "            if 1 not in pred:\n",
    "                # 全部不分割\n",
    "                #print('no')\n",
    "                seg_sents_num = streaming_input_ids.count(21128)\n",
    "                seg_sents_num_str = str(seg_sents_num)+'+'\n",
    "            else:\n",
    "                first_seg = pred.index(1)-1#.tolist().count(1)\n",
    "                # first_seg: 第一段最后一个token id\n",
    "                # 按照分割位置计算前方句子数量\n",
    "                seg_sents_num = streaming_input_ids[:first_seg+1].count(21128)\n",
    "                seg_sents_num_str = str(seg_sents_num)\n",
    "            if seg_sents_num == 0:\n",
    "                #print('no_0', end='')\n",
    "                seg_sents_num = streaming_input_ids.count(21128)\n",
    "                seg_sents_num_str = str(seg_sents_num)+'+'\n",
    "            block_length.append(seg_sents_num)\n",
    "            block_length_merge.append(seg_sents_num_str)\n",
    "            #print(seg_sents_num,block_length, sum(block_length))\n",
    "        #print(block_length)\n",
    "        #print(block_length_merge)\n",
    "        # 当前段落没有分割。与下个段落合并\n",
    "        i, j = 0, 0\n",
    "        while i < len(block_length_merge)-1: # 最后一个不管\n",
    "            if '+' not in block_length_merge[i]:\n",
    "                i+=1\n",
    "                j+=1\n",
    "            else:\n",
    "                block_length[j] += block_length[j+1]\n",
    "                block_length.pop(j+1)\n",
    "                i+=2\n",
    "                j+=1\n",
    "            if j >= len(block_length)-1:\n",
    "                break\n",
    "        #print(block_length)\n",
    "        test_pred_block_length.append(block_length)\n",
    "    \n",
    "    cal_res = []\n",
    "    for i in range(len(test_pred_block_length)):\n",
    "        pred = test_pred_block_length[i]\n",
    "        label = test_label_block_length[i]\n",
    "        cal_res.append(cal_seg_eval(pred, label))\n",
    "\n",
    "    res = {k:sum([i[k] for i in cal_res])/len(cal_res) for k,v in cal_res[0].items()}\n",
    "    res['f1'] = cal_label_pred_f1_block(test_label_block_length, test_pred_block_length)['EOP']['f1']\n",
    "    print(res)\n",
    "    return res, test_pred_block_length, test_label_block_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63473951",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ws in range(40,150,10):\n",
    "    test_res, test_pred_b, test_label_b = test(window_size=ws, stop_block_size=23)\n",
    "\n",
    "    cal_res = cal_segeval(test_pred_b, \n",
    "                          test_label_b, \n",
    "                          tolerate_dist=21, \n",
    "                          type='block', \n",
    "                          end_process=True, \n",
    "                          decimal_places=2)\n",
    "    print(ws,'\\n',cal_res)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rhe2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
